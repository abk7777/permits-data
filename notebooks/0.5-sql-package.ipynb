{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Package\n",
    "\n",
    "Provides simple functionality to interact with a PostgreSQL server using Python classes.\n",
    "\n",
    "**Overview of functionality:**\n",
    "* Database(self, user, password, host, dbname, port)\n",
    "    * properties\n",
    "        * user\n",
    "        * password\n",
    "        * host\n",
    "        * dbname\n",
    "        * port\n",
    "    * methods\n",
    "        * create(name) x\n",
    "        * connect()\n",
    "        * drop(name)\n",
    "* Table(self, dbname, table, schema)\n",
    "    * accepts db properties\n",
    "    * properties\n",
    "        * connect() --> inherited\n",
    "        * fetch_data(sql, con, parse_dates)\n",
    "        * get_names()\n",
    "        * format_names(char_dict)\n",
    "        * update_names(names_dict)\n",
    "        * add_columns(columns_list, type=None)\n",
    "        * compare_column_order(dataframe)\n",
    "        * match_columns(dataframe)\n",
    "        * save_csv(data, local_path, match_column_order=True)\n",
    "        * update_values(local_path, container_path)\n",
    "        * update_types(types_dict)\n",
    "        * close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "#sys.path[0] = str(Path(__file__).resolve().parents[2]) # Set path for custom modules\n",
    "import warnings\n",
    "from io import StringIO\n",
    "\n",
    "# Set path for modules\n",
    "sys.path[0] = '../'\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# SQL libraries\n",
    "import psycopg2\n",
    "\n",
    "# Set notebook display options\n",
    "pd.set_option('display.max_rows', 2000)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Get project root directory\n",
    "root_dir = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Database():\n",
    "    \n",
    "    # if modulename not in sys.modules: print...\n",
    "    load_dotenv(find_dotenv());\n",
    "    \n",
    "    def __init__(self, user=None, password=None,\n",
    "                 dbname=None, host=None, port=None):\n",
    "        \n",
    "        # Loaded from .env if not explicit\n",
    "        self.user = user if user is not None else os.getenv(\"POSTGRES_USER\")\n",
    "        self.password = password if password is not None else os.getenv(\"POSTGRES_PASSWORD\")\n",
    "        self.dbname = dbname if dbname is not None else os.getenv(\"POSTGRES_DB\")\n",
    "        self.host = host if host is not None else os.getenv(\"DB_HOST\")\n",
    "        self.port = port if port is not None else os.getenv(\"DB_PORT\")\n",
    "        \n",
    "        \n",
    "        # Root directory\n",
    "        self._root_dir = os.path.dirname(os.getcwd())\n",
    "        #sys.path[0] = str(Path(__file__).resolve().parents[2])\n",
    "        \n",
    "    def _connect(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Connects to PostgreSQL database using psycopg2 driver. Same\n",
    "        arguments as psycopg2.connect().\n",
    "\n",
    "        Params\n",
    "        --------\n",
    "        dbname\n",
    "        user\n",
    "        password\n",
    "        host\n",
    "        port\n",
    "        connect_timeout\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            con = psycopg2.connect(dbname=self.dbname,\n",
    "                                   user=self.user,\n",
    "                                   password=self.password,\n",
    "                                    host=self.host, \n",
    "                                    port=self.port,\n",
    "                                  connect_timeout=3)            \n",
    "\n",
    "        except Exception as e:\n",
    "            print('Error:\\n', e)\n",
    "            return None\n",
    "\n",
    "\n",
    "        return con\n",
    "    \n",
    "    @property\n",
    "    def _con(self):\n",
    "        try:\n",
    "            con = self._connect()\n",
    "            print('Connected as user \"{}\" to database \"{}\" on http://{}:{}.'.format(self.user,self.dbname,\n",
    "                                                               self.host,self.port))\n",
    "            con.close()\n",
    "        except Exception as e:\n",
    "            con.rollback()\n",
    "            print('Error:\\n', e)\n",
    "        finally:\n",
    "            if con is not None:\n",
    "                con.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Table(Database):\n",
    "    def __init__(self, user=None, password=None, dbname=None, host=None, port=None, table=None):\n",
    "        super().__init__(user, password, dbname, host, port)\n",
    "        \n",
    "        self.table = table\n",
    "        \n",
    "        # Loaded from .env if not explicit\n",
    "        self.user = user if user is not None else os.getenv(\"POSTGRES_USER\")\n",
    "        self.password = password if password is not None else os.getenv(\"POSTGRES_PASSWORD\")\n",
    "        self.dbname = dbname if dbname is not None else os.getenv(\"POSTGRES_DB\")\n",
    "        self.host = host if host is not None else os.getenv(\"DB_HOST\")\n",
    "        self.port = port if port is not None else os.getenv(\"DB_PORT\")\n",
    "    \n",
    "    # Connect to database\n",
    "    def __connect(self):\n",
    "        return super(Table, self)._connect()\n",
    "    \n",
    "    # Check info on connection\n",
    "    def __con(self):\n",
    "        return super(Table, self)._con\n",
    "    \n",
    "    # Fetch data from sql query\n",
    "    def fetch_data(self, sql, coerce_float=False, parse_dates=None):\n",
    "        \n",
    "        con = self.__connect()\n",
    "        \n",
    "        # Fetch fresh data\n",
    "        data = pd.read_sql_query(sql=sql, con=con, coerce_float=coerce_float, parse_dates=parse_dates)\n",
    "\n",
    "        # Replace None with np.nan\n",
    "        data.fillna(np.nan, inplace=True)\n",
    "        \n",
    "        # Close db connection\n",
    "        con.close()\n",
    "\n",
    "        return data\n",
    "    \n",
    "    # Get names of column\n",
    "    def get_names(self):\n",
    "        \n",
    "        # Specific query to retrieve table names\n",
    "        sql = \"SELECT * FROM information_schema.columns WHERE table_name = N'{}'\".format(self.table)\n",
    "        \n",
    "        # Run query and extract\n",
    "        con = self.__connect()\n",
    "        data = pd.read_sql_query(sql, con)\n",
    "        column_series = data['column_name']\n",
    "        con.close()\n",
    "    \n",
    "        return column_series\n",
    "    \n",
    "    # Get types of columns, returns dict\n",
    "    def get_types(self):\n",
    "        \n",
    "        # Specific query to retrieve table names\n",
    "        sql = '''SELECT column_name, \n",
    "        CASE \n",
    "            WHEN domain_name is not null then domain_name\n",
    "            WHEN data_type='character varying' THEN 'varchar('||character_maximum_length||')'\n",
    "            WHEN data_type='character' THEN 'char('||character_maximum_length||')'\n",
    "            WHEN data_type='numeric' THEN 'numeric'\n",
    "            ELSE data_type\n",
    "        END AS type\n",
    "        FROM information_schema.columns WHERE table_name = 'permits_raw';\n",
    "        '''\n",
    "        \n",
    "        # Run query and extract\n",
    "        con = self.__connect()\n",
    "        data = pd.read_sql_query(sql, con)\n",
    "        con.close()\n",
    "        \n",
    "        types_dict = dict(zip(data['column_name'], data['type'].str.upper()))\n",
    "        \n",
    "        return types_dict\n",
    "\n",
    "    # Standardize column names using dictionary of character replacements\n",
    "    def reformat_names(self, replace_map):\n",
    "        \n",
    "        series = self.get_names()\n",
    "        \n",
    "        def replace_chars(text):\n",
    "            for oldchar, newchar in replace_map.items():\n",
    "                text = text.replace(oldchar, newchar).lower()\n",
    "            return text\n",
    "        \n",
    "        return series.apply(replace_chars)\n",
    "    \n",
    "    # Update column names in db table\n",
    "    def update_names(self, replace_map):\n",
    "        \n",
    "        # Extract current columns in table\n",
    "        old_columns = self.get_names()\n",
    "        \n",
    "        # Create list of reformatted columns to replace old columns \n",
    "        new_columns = self.reformat_names(replace_map)\n",
    "    \n",
    "        # SQL query string to change column names\n",
    "        sql = 'ALTER TABLE {} '.format(self.table) + 'RENAME \"{old_name}\" to {new_name};'\n",
    "\n",
    "        sql_query = []\n",
    "\n",
    "        # Iterate through old column names and replace each with reformatted name \n",
    "        for idx, name in old_columns.iteritems():\n",
    "            sql_query.append(sql.format(old_name=name, new_name=new_columns[idx]))\n",
    "            \n",
    "        # Join list to string\n",
    "        sql_query = '\\n'.join(sql_query)\n",
    "        \n",
    "        # Execute query against database\n",
    "        con = self.__connect()\n",
    "        try:\n",
    "            cur = con.cursor()\n",
    "            cur.execute(sql_query)\n",
    "            con.commit()\n",
    "            cur.close()\n",
    "            print('Updated table \"{}\".'.format(self.table))\n",
    "        except Exception as e:\n",
    "            con.rollback()\n",
    "            print('Error:\\n', e)\n",
    "        finally:\n",
    "            if con is not None:\n",
    "                con.close()\n",
    "                \n",
    "    # Add new columns to database\n",
    "    def add_columns(self, data):\n",
    "\n",
    "        # Get names of current columns in PostgreSQL table\n",
    "        current_names = self.get_names()\n",
    "\n",
    "        # Get names of updated table not in current table\n",
    "        updated_names = data.columns.tolist()\n",
    "        new_names = list(set(updated_names) - set(current_names))\n",
    "\n",
    "        # Check names list is not empty\n",
    "        if not new_names:\n",
    "            print(\"Table columns are already up to date.\")\n",
    "            return\n",
    "\n",
    "        # Format strings for query\n",
    "        alter_table_sql = \"ALTER TABLE {db_table}\\n\"\n",
    "        add_column_sql = \"\\tADD COLUMN {column} TEXT,\\n\"\n",
    "\n",
    "        # Create a list and append ADD column statements\n",
    "        sql_query = [alter_table_sql.format(db_table=self.table)]\n",
    "        for name in new_names:\n",
    "            sql_query.append(add_column_sql.format(column=name))\n",
    "\n",
    "        # Join into one string\n",
    "        sql_query = ''.join(sql_query)[:-2] + \";\"\n",
    "\n",
    "        # Execute query against database\n",
    "        con = self.__connect()\n",
    "        try:\n",
    "            cur = con.cursor()\n",
    "            cur.execute(sql_query)\n",
    "            con.commit()\n",
    "            cur.close()\n",
    "            print('Updated table \"{}\".'.format(self.table))\n",
    "        except Exception as e:\n",
    "            con.rollback()\n",
    "            print('Error:\\n', e)\n",
    "        finally:\n",
    "            if con is not None:\n",
    "                con.close()\n",
    "\n",
    "    # Compare order of columns in dataframe against order of columns in database                \n",
    "    def compare_column_order(self, data):\n",
    "        \n",
    "        # Get columns from database as list\n",
    "        db_columns = self.get_names().tolist()\n",
    "        \n",
    "        # Select columns from dataframe as list\n",
    "        data_columns = data.columns.tolist()\n",
    "        \n",
    "        if set(data_columns) == set(db_columns):\n",
    "            \n",
    "            str1 = 'Dataframe columns match table \"{}\" '.format(self.table)\n",
    "            \n",
    "            if data_columns == db_columns:\n",
    "                print(str1 + \"and are in identical order.\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"but are not in identical order.\")                \n",
    "                return False            \n",
    "        else:\n",
    "            if len(data_columns) > len(db_columns):\n",
    "                print('Dataframe has more columns than table \"{}\".'.format(self.table))\n",
    "                return False\n",
    "            else:\n",
    "                print('Dataframe has less columns than table \"{}\".'.format(self.table))\n",
    "                return False\n",
    "    \n",
    "    # Rearrange the order of columns in dataframe to match order in table\n",
    "    def match_column_order(self, data):\n",
    "        \n",
    "        # Get columns from database as list\n",
    "        db_columns = self.get_names().tolist()\n",
    "\n",
    "        # Select columns from dataframe as list\n",
    "        data_columns = data.columns.tolist()\n",
    "        \n",
    "        if set(data_columns) == set(db_columns):\n",
    "            if data_columns != db_columns:\n",
    "                print('Rearranged dataframe columns to match table \"{}\".'.format(self.table))\n",
    "                return data[db_columns]\n",
    "            else:\n",
    "                print('Dataframe columns already match table \"{}\".'.format(self.table))\n",
    "                return data\n",
    "        else:\n",
    "            if len(data_columns) > len(db_columns):\n",
    "                print('Dataframe has more columns than table \"{}\".'.format(self.table))\n",
    "                return data\n",
    "            else:\n",
    "                print('Dataframe has less columns than table \"{}\".'.format(self.table))\n",
    "                return data\n",
    "            \n",
    "    # Builds a query to update postgres from a csv file\n",
    "    def update_values(self, data, id_col, columns=None, sep=','):\n",
    "        \n",
    "        # Fetch data types\n",
    "        types_dict = self.get_types()\n",
    "        \n",
    "        # Append id_col to selected columns\n",
    "        columns = None if not columns else [id_col] + columns\n",
    "        \n",
    "        # CREATE TABLE query\n",
    "        tmp_table = \"tmp_\" + self.table\n",
    "\n",
    "        column_names = self.get_names().tolist() if not columns else columns\n",
    "        \n",
    "        # Subsets types_dict by columns argument and formats into string\n",
    "        types_dict = types_dict if not columns else {key:value for key, value in types_dict.items() if key in set(columns)}\n",
    "        names = ',\\n\\t'.join(['{key} {val}'.format(key=key, val=val) for key, val in types_dict.items()])\n",
    "        \n",
    "        # Build queries\n",
    "        sql_create_tmp_table = 'DROP TABLE IF EXISTS {};\\n\\n'.format(tmp_table)\n",
    "        sql_create_tmp_table = sql_create_tmp_table + 'CREATE TABLE {tmp_table} (\\n\\t{names}\\n);\\n\\n' \\\n",
    "                                .format(tmp_table=tmp_table, names=names)\n",
    "           \n",
    "        sql_update_query = 'UPDATE {db_table}\\n'.format(db_table=self.table)\n",
    "        \n",
    "        sql_set = [\"SET \"]\n",
    "        \n",
    "        for name in column_names:\n",
    "            set_sql = \"{name} = {tmp_name},\\n\\t\".format(name=name, tmp_name=tmp_table + '.' + name)\n",
    "            sql_set.append(set_sql)\n",
    "            \n",
    "        sql_set = ''.join(sql_set)\n",
    "        sql_set = sql_set[:-3] + \"\\n\"\n",
    "        \n",
    "        sql_from = \"FROM {tmp_table}\\nWHERE {db_table}.{id_col} = {tmp_table}.{id_col};\\n\\n\" \\\n",
    "                            .format(tmp_table=tmp_table, db_table=self.table, id_col=id_col)\n",
    "        sql_drop = 'DROP TABLE {};\\n'.format(tmp_table)\n",
    "        \n",
    "        sql_query_1 = sql_create_tmp_table\n",
    "        sql_query_2 = sql_update_query + sql_set + sql_from + sql_drop\n",
    "\n",
    "        # Preview sql query to debug\n",
    "        #print(sql_query_1 + \"# Copy into temp_table\\ncur.copy_from(...)\\n\\n\"+ sql_query_2)\n",
    "        \n",
    "        # Run update query\n",
    "        data_buffer = StringIO(data.to_csv(header=False, index=False))\n",
    "        con = self.__connect()\n",
    "        try:\n",
    "            cur = con.cursor()\n",
    "            \n",
    "            # Create tmp_table\n",
    "            cur.execute(sql_query_1)\n",
    "\n",
    "            # Copy into temp_table\n",
    "            data_buffer.read()\n",
    "            cur.copy_from(file=data_buffer, table=tmp_table, columns=columns, sep=sep)\n",
    "            data_buffer.close()\n",
    "            \n",
    "            # Update from temp_table into table and delete temp\n",
    "            cur.execute(sql_query_2)\n",
    "            con.commit()\n",
    "            cur.close()\n",
    "            print('Updated table \"{}\".'.format(self.table))\n",
    "        except Exception as e:\n",
    "            con.rollback()\n",
    "            print('Error:\\n', e)\n",
    "        finally:\n",
    "            if con is not None:\n",
    "                con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "permits = Table(table=\"permits_raw\")\n",
    "data = permits.fetch_data(sql=\"SELECT * FROM permits_raw;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated table \"permits_raw\".\n"
     ]
    }
   ],
   "source": [
    "permits.update_values(data, id_col=\"pcis_permit_no\", columns=['assessor_book', 'latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rewrite to save table as csv, not dataframe\n",
    "# Save csv with option to match order of columns in postgres\n",
    "def save_csv(data, path, index=False):\n",
    "\n",
    "    # Check unique columns\n",
    "    if data.columns.tolist() != data.columns.unique().tolist():\n",
    "        raise IndexError(\"Dataframe has duplicate columns.\")\n",
    "\n",
    "    if index:\n",
    "        warnings.warn('Setting \"index=True\" may cause problems when importing from csv file.')\n",
    "\n",
    "\n",
    "    # Write to csv\n",
    "    data.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected as user \"postgres\" to database \"permits\" on http://localhost:5432.\n"
     ]
    }
   ],
   "source": [
    "permits._con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      assessor_book\n",
       "1      assessor_page\n",
       "2    assessor_parcel\n",
       "Name: column_name, dtype: object"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permits.get_names()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      assessor_book\n",
       "1      assessor_page\n",
       "2    assessor_parcel\n",
       "Name: column_name, dtype: object"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map of character replacements\n",
    "replace_map = {' ': '_', '-': '_', '#': 'No', '/': '_', \n",
    "               '.': '', '(': '', ')': '', \"'\": ''}\n",
    "\n",
    "permits.reformat_names(replace_map)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting...\n",
      "Executing query on table \"permits_raw\"...\n",
      "Error:\n",
      " column \"assessor_book\" of relation \"permits_raw\" already exists\n",
      "\n"
     ]
    }
   ],
   "source": [
    "permits.update_names(replace_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table columns are already up to date.\n"
     ]
    }
   ],
   "source": [
    "permits.add_columns(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      assessor_book\n",
       "1      assessor_page\n",
       "2    assessor_parcel\n",
       "Name: column_name, dtype: object"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permits.get_names()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe columns match table \"permits_raw\" and are in identical order.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permits.compare_column_order(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe columns already match table \"permits_raw\".\n"
     ]
    }
   ],
   "source": [
    "permits.match_column_order(data).head();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/anaconda3/envs/permits-data-env/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Setting \"index=True\" may cause problems when importing from csv file.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "path = root_dir + \"/data/interim/test.csv\"\n",
    "\n",
    "save_csv(data, path, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_dict = {'status':'VARCHAR(50)', 'permit_type':'VARCHAR(50)', 'permit_sub_type':'VARCHAR(50)', \n",
    "                'permit_category':'VARCHAR(50)', 'initiating_office':'VARCHAR(50)', \n",
    "                'license_type':'VARCHAR(50)', 'zone':'VARCHAR(50)', 'census_tract':'VARCHAR(50)', \n",
    "                'applicant_relationship':'VARCHAR(50)', 'block':'VARCHAR(50)', 'lot':'VARCHAR(50)', \n",
    "                'reference_no_old_permit_no':'VARCHAR(50)','pcis_permit_no':'VARCHAR(50)', \n",
    "               'address_fraction_start': 'CHAR(3)', 'address_fraction_end': 'CHAR(3)', \n",
    "                'street_direction': 'CHAR(1)', 'street_name': 'VARCHAR(50)', 'street_suffix': 'VARCHAR(10)',\n",
    "               'suffix_direction': 'VARCHAR(10)', 'unit_range_start': 'VARCHAR(50)', 'unit_range_end': 'VARCHAR(50)',\n",
    "               'work_description': 'TEXT', 'floor_area_la_zoning_code_definition': 'VARCHAR(10)', \n",
    "               'contractors_business_name': 'VARCHAR(100)', 'contractor_address': 'VARCHAR(100)',\n",
    "               'contractor_city': 'VARCHAR(50)', 'contractor_state': 'CHAR(2)', 'license_type': 'VARCHAR(10)', \n",
    "               'principal_first_name': 'VARCHAR(50)', 'principal_middle_name': 'VARCHAR(50)', \n",
    "                'principal_last_name': 'VARCHAR(50)', 'applicant_first_name': 'VARCHAR(50)', \n",
    "                'applicant_last_name': 'VARCHAR(50)', 'applicant_business_name': 'VARCHAR(100)',\n",
    "               'applicant_address_1': 'VARCHAR(50)', 'applicant_address_2': 'VARCHAR(50)', \n",
    "                'applicant_address_3': 'VARCHAR(50)', 'occupancy': 'VARCHAR(50)', \n",
    "                'floor_area_la_building_code_definition': 'VARCHAR(10)', 'census_tract': 'VARCHAR(10)',\n",
    "                'latitude_longitude': 'VARCHAR(50)', 'assessor_parcel': 'CHAR(3)', 'tract': 'VARCHAR(200)',\n",
    "              'assessor_book': 'SMALLINT', 'assessor_page': 'SMALLINT', 'council_district': 'SMALLINT', \n",
    "                'project_number': 'SMALLINT', 'address_start': 'INTEGER', \n",
    "                'address_end': 'INTEGER', 'no_of_residential_dwelling_units': 'SMALLINT', \n",
    "                'no_of_accessory_dwelling_units': 'SMALLINT', 'no_of_stories': 'SMALLINT', \n",
    "                'license_no': 'INTEGER', 'zip_code': 'INTEGER', 'existing_code': 'SMALLINT', \n",
    "                'proposed_code': 'SMALLINT', 'valuation':'NUMERIC', 'latitude':'NUMERIC', 'longitude':'NUMERIC',\n",
    "             'status_date': 'DATE', 'issue_date': 'DATE', 'license_expiration_date': 'DATE', 'event_code':'VARCHAR(50)',\n",
    "             'full_address':'VARCHAR(100)'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-416-974e71b4a37a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msql\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpermits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pcis_permit_no'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#print(sql)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#for key, value in types_dict.items():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#print(''.join([key, ' ', value, ',']))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-412-2d577f7606ec>\u001b[0m in \u001b[0;36mupdate_values\u001b[0;34m(self, data, id_col, columns, sep)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mdb_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mdb_data_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0mtypes_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_data_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gregory/anaconda3/envs/permits-data-env/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": [
    "sql = permits.update_values(data, id_col='pcis_permit_no')\n",
    "#print(sql)\n",
    "\n",
    "#for key, value in types_dict.items():\n",
    "    #print(''.join([key, ' ', value, ',']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permits_fetch_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
