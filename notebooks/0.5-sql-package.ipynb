{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Package\n",
    "\n",
    "Provides simple functionality to interact with a PostgreSQL server using Python classes.\n",
    "\n",
    "**Overview of functionality:**\n",
    "* Database(self, user, password, host, dbname, port)\n",
    "    * properties\n",
    "        * user\n",
    "        * password\n",
    "        * host\n",
    "        * dbname\n",
    "        * port\n",
    "    * methods\n",
    "        * create(name) x\n",
    "        * connect()\n",
    "        * drop(name)\n",
    "* Table(self, dbname, table, schema)\n",
    "    * accepts db properties\n",
    "    * properties\n",
    "        * connect() --> inherited\n",
    "        * fetch_data(sql, con, parse_dates)\n",
    "        * get_names()\n",
    "        * format_names(char_dict)\n",
    "        * update_names(names_dict)\n",
    "        * add_columns(columns_list, type=None)\n",
    "        * compare_column_order(dataframe)\n",
    "        * match_columns(dataframe)\n",
    "        * save_csv(data, local_path, match_column_order=True)\n",
    "        * update_values(local_path, container_path)\n",
    "        * update_types(types_dict)\n",
    "        * close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "#sys.path[0] = str(Path(__file__).resolve().parents[2]) # Set path for custom modules\n",
    "import warnings\n",
    "from io import StringIO\n",
    "\n",
    "# Set path for modules\n",
    "sys.path[0] = '../'\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# SQL libraries\n",
    "import psycopg2\n",
    "from src.pipeline.database import types_dict, replace_map\n",
    "from src.pipeline.transform_data import create_column_full_address, geocode_latitude_longitude, split_column_lat_long\n",
    "from src.toolkits.sql import format_names\n",
    "\n",
    "# Set notebook display options\n",
    "pd.set_option('display.max_rows', 2000)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Get project root directory\n",
    "root_dir = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Maps environment variables\n",
    "#load_dotenv(find_dotenv());\n",
    "#GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "#GOOGLE_AGENT = os.getenv(\"GOOGLE_AGENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Database():\n",
    "    \n",
    "    # if modulename not in sys.modules: print...\n",
    "    load_dotenv(find_dotenv());\n",
    "    \n",
    "    def __init__(self, user=None, password=None,\n",
    "                 dbname=None, host=None, port=None):\n",
    "        \n",
    "        # Loaded from .env if not explicit\n",
    "        self.user = user if user is not None else os.getenv(\"POSTGRES_USER\")\n",
    "        self.password = password if password is not None else os.getenv(\"POSTGRES_PASSWORD\")\n",
    "        self.dbname = dbname if dbname is not None else os.getenv(\"POSTGRES_DB\")\n",
    "        self.host = host if host is not None else os.getenv(\"DB_HOST\")\n",
    "        self.port = port if port is not None else os.getenv(\"DB_PORT\")\n",
    "        \n",
    "        \n",
    "        # Root directory\n",
    "        self._root_dir = os.path.dirname(os.getcwd())\n",
    "        #sys.path[0] = str(Path(__file__).resolve().parents[2])\n",
    "        \n",
    "    def _connect(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Connects to PostgreSQL database using psycopg2 driver. Same\n",
    "        arguments as psycopg2.connect().\n",
    "\n",
    "        Params\n",
    "        --------\n",
    "        dbname\n",
    "        user\n",
    "        password\n",
    "        host\n",
    "        port\n",
    "        connect_timeout\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            con = psycopg2.connect(dbname=self.dbname,\n",
    "                                   user=self.user,\n",
    "                                   password=self.password,\n",
    "                                    host=self.host, \n",
    "                                    port=self.port,\n",
    "                                  connect_timeout=3)            \n",
    "\n",
    "        except Exception as e:\n",
    "            print('Error:\\n', e)\n",
    "            return None\n",
    "\n",
    "\n",
    "        return con\n",
    "    \n",
    "    @property\n",
    "    def _con(self):\n",
    "        try:\n",
    "            con = self._connect()\n",
    "            print('Connected as user \"{}\" to database \"{}\" on http://{}:{}.'.format(self.user,self.dbname,\n",
    "                                                               self.host,self.port))\n",
    "            con.close()\n",
    "        except Exception as e:\n",
    "            con.rollback()\n",
    "            print('Error:\\n', e)\n",
    "        finally:\n",
    "            if con is not None:\n",
    "                con.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Table(Database):\n",
    "    def __init__(self, user=None, password=None, dbname=None, host=None, port=None, table=None):\n",
    "        super().__init__(user, password, dbname, host, port)\n",
    "        \n",
    "        self.table = table\n",
    "        \n",
    "        # Loaded from .env if not explicit\n",
    "        self.user = user if user is not None else os.getenv(\"POSTGRES_USER\")\n",
    "        self.password = password if password is not None else os.getenv(\"POSTGRES_PASSWORD\")\n",
    "        self.dbname = dbname if dbname is not None else os.getenv(\"POSTGRES_DB\")\n",
    "        self.host = host if host is not None else os.getenv(\"DB_HOST\")\n",
    "        self.port = port if port is not None else os.getenv(\"DB_PORT\")\n",
    "    \n",
    "    # Connect to database\n",
    "    def __connect(self):\n",
    "        return super(Table, self)._connect()\n",
    "    \n",
    "    # Check info on connection\n",
    "    def __con(self):\n",
    "        return super(Table, self)._con\n",
    "    \n",
    "    # Fetch data from sql query\n",
    "    def fetch_data(self, sql, coerce_float=False, parse_dates=None):\n",
    "        \n",
    "        con = self.__connect()\n",
    "        \n",
    "        # Fetch fresh data\n",
    "        data = pd.read_sql_query(sql=sql, con=con, coerce_float=coerce_float, parse_dates=parse_dates)\n",
    "\n",
    "        # Replace None with np.nan\n",
    "        data.fillna(np.nan, inplace=True)\n",
    "        \n",
    "        # Close db connection\n",
    "        con.close()\n",
    "\n",
    "        return data\n",
    "    \n",
    "    # Get names of column\n",
    "    def get_names(self):\n",
    "        \n",
    "        # Specific query to retrieve table names\n",
    "        sql = \"SELECT * FROM information_schema.columns WHERE table_name = N'{}'\".format(self.table)\n",
    "        \n",
    "        # Run query and extract\n",
    "        con = self.__connect()\n",
    "        data = pd.read_sql_query(sql, con)\n",
    "        column_series = data['column_name']\n",
    "        con.close()\n",
    "    \n",
    "        return column_series\n",
    "    \n",
    "    # Get types of columns, returns dict\n",
    "    def get_types(self, as_dataframe=False):\n",
    "        \n",
    "        # Specific query to retrieve table names\n",
    "        sql = '''SELECT column_name, \n",
    "        CASE \n",
    "            WHEN domain_name is not null then domain_name\n",
    "            WHEN data_type='character varying' THEN 'varchar('||character_maximum_length||')'\n",
    "            WHEN data_type='character' THEN 'char('||character_maximum_length||')'\n",
    "            WHEN data_type='numeric' THEN 'numeric'\n",
    "            ELSE data_type\n",
    "        END AS type\n",
    "        FROM information_schema.columns WHERE table_name = 'permits_raw';\n",
    "        '''\n",
    "        \n",
    "        # Run query and extract\n",
    "        con = self.__connect()\n",
    "        data = pd.read_sql_query(sql, con)\n",
    "        con.close()\n",
    "        \n",
    "        if as_dataframe:\n",
    "            data['type'] = data['type'].str.upper()\n",
    "            return data\n",
    "        \n",
    "        types_dict = dict(zip(data['column_name'], data['type'].str.upper()))\n",
    "        \n",
    "        return types_dict\n",
    "\n",
    "    # Standardize column names using dictionary of character replacements\n",
    "    def reformat_names(self, replace_map, update=False):\n",
    "        \n",
    "        series = self.get_names()\n",
    "        \n",
    "        def replace_chars(text):\n",
    "            for oldchar, newchar in replace_map.items():\n",
    "                text = text.replace(oldchar, newchar).lower()\n",
    "            return text\n",
    "        \n",
    "        series = series.apply(replace_chars)\n",
    "        \n",
    "        # Update column names in db table\n",
    "        def update_names(series):\n",
    "\n",
    "            # Extract current columns in table\n",
    "            old_columns = self.get_names()\n",
    "\n",
    "            # Create list of reformatted columns to replace old columns \n",
    "            new_columns = series\n",
    "\n",
    "            # SQL query string to change column names\n",
    "            sql = 'ALTER TABLE {} '.format(self.table) + 'RENAME \"{old_name}\" to {new_name};'\n",
    "\n",
    "            sql_query = []\n",
    "\n",
    "            # Iterate through old column names and replace each with reformatted name \n",
    "            for idx, name in old_columns.iteritems():\n",
    "                sql_query.append(sql.format(old_name=name, new_name=new_columns[idx]))\n",
    "\n",
    "            # Join list to string\n",
    "            sql_query = '\\n'.join(sql_query)\n",
    "\n",
    "            return sql_query  \n",
    "        \n",
    "        if not update:\n",
    "            return series.apply(replace_chars)\n",
    "        \n",
    "        else:\n",
    "            sql_query = update_names(series=series)\n",
    "            \n",
    "            # Execute query against database\n",
    "            con = self.__connect()\n",
    "            try:\n",
    "                cur = con.cursor()\n",
    "                cur.execute(sql_query)\n",
    "                con.commit()\n",
    "                cur.close()\n",
    "                print('Updated table \"{}\".'.format(self.table))\n",
    "            except Exception as e:\n",
    "                con.rollback()\n",
    "                print('Error:\\n', e)\n",
    "            finally:\n",
    "                if con is not None:\n",
    "                    con.close()\n",
    "                    \n",
    "\n",
    "    # Add new columns to database\n",
    "    def add_columns(self, add_from):\n",
    "\n",
    "        con = self.__connect()\n",
    "        \n",
    "        # Get names of current columns in PostgreSQL table\n",
    "        current_names = self.get_names()\n",
    "\n",
    "        # Get names of updated table not in current table\n",
    "        updated_names = data.columns.tolist()\n",
    "        new_names = list(set(updated_names) - set(current_names))\n",
    "\n",
    "        # Check names list is not empty\n",
    "        if not new_names:\n",
    "            print(\"Table columns are already up to date.\")\n",
    "            return\n",
    "\n",
    "        # Format strings for query\n",
    "        alter_table_sql = \"ALTER TABLE {db_table}\\n\"\n",
    "        add_column_sql = \"\\tADD COLUMN {column} TEXT,\\n\"\n",
    "\n",
    "        # Create a list and append ADD column statements\n",
    "        sql_query = [alter_table_sql.format(db_table=self.table)]\n",
    "        for name in new_names:\n",
    "            sql_query.append(add_column_sql.format(column=name))\n",
    "\n",
    "        # Join into one string\n",
    "        sql_query = ''.join(sql_query)[:-2] + \";\"\n",
    "\n",
    "        # Execute query against database\n",
    "        try:\n",
    "            cur = con.cursor()\n",
    "            cur.execute(sql_query)\n",
    "            con.commit()\n",
    "            cur.close()\n",
    "            print('Updated table \"{}\".'.format(self.table))\n",
    "        except Exception as e:\n",
    "            con.rollback()\n",
    "            print('Error:\\n', e)\n",
    "        finally:\n",
    "            if con is not None:\n",
    "                con.close()\n",
    "\n",
    "    # Compare order of columns in dataframe against order of columns in database                \n",
    "    def compare_column_order(self, data):\n",
    "        \n",
    "        # Get columns from database as list\n",
    "        db_columns = self.get_names().tolist()\n",
    "        \n",
    "        # Select columns from dataframe as list\n",
    "        data_columns = data.columns.tolist()\n",
    "        \n",
    "        if set(data_columns) == set(db_columns):\n",
    "            \n",
    "            str1 = 'Dataframe columns match table \"{}\" '.format(self.table)\n",
    "            \n",
    "            if data_columns == db_columns:\n",
    "                print(str1 + \"and are in identical order.\")\n",
    "                return True\n",
    "            else:\n",
    "                print(str1 + \"but are not in identical order.\")                \n",
    "                return False            \n",
    "        else:\n",
    "            if len(data_columns) > len(db_columns):\n",
    "                print('Dataframe has columns not in table \"{}\":'.format(self.table))\n",
    "                return list(set(data_columns) - set(db_columns))\n",
    "            else:\n",
    "                print('Dataframe missing columns that are in table \"{}\":'.format(self.table))\n",
    "                return list(set(db_columns) - set(data_columns))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Rearrange the order of columns in dataframe to match order in table\n",
    "    def match_column_order(self, data):\n",
    "        \n",
    "        # Get columns from database as list\n",
    "        db_columns = self.get_names().tolist()\n",
    "\n",
    "        # Select columns from dataframe as list\n",
    "        data_columns = data.columns.tolist()\n",
    "        \n",
    "        if set(data_columns) == set(db_columns):\n",
    "            if data_columns != db_columns:\n",
    "                print('Rearranged dataframe columns to match table \"{}\".'.format(self.table))\n",
    "                return data[db_columns]\n",
    "            else:\n",
    "                print('Dataframe columns already match table \"{}\".'.format(self.table))\n",
    "                return data\n",
    "        else:\n",
    "            if len(data_columns) > len(db_columns):\n",
    "                print('Dataframe has columns not in table \"{}\":'.format(self.table))\n",
    "                return list(set(data_columns) - set(db_columns))\n",
    "            else:\n",
    "                print('Dataframe missing columns that are in table \"{}\":'.format(self.table))\n",
    "                return list(set(db_columns) - set(data_columns))\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    def _create_temp_table(self, types_dict, id_col, columns=None):\n",
    "        \n",
    "        # Fetch data types\n",
    "        #types_dict = types_dict\n",
    "        \n",
    "        # Append id_col to selected columns\n",
    "        columns = None if not columns else [id_col] + columns\n",
    "        \n",
    "        # CREATE TABLE query\n",
    "        tmp_table = \"tmp_\" + self.table\n",
    "\n",
    "        #column_names = self.get_names().tolist() if not columns else columns\n",
    "        \n",
    "        # Subsets types_dict by columns argument and formats into string if no columns are specified\n",
    "        types_dict = types_dict if not columns else {key:value for key, value in types_dict.items() if key in set(columns)}\n",
    "        names = ',\\n\\t'.join(['{key} {val}'.format(key=key, val=val) for key, val in types_dict.items()])\n",
    "        \n",
    "        # Build queries\n",
    "        sql = 'DROP TABLE IF EXISTS {tmp_table};\\n\\n'.format(tmp_table=tmp_table)\n",
    "        sql = sql + 'CREATE TABLE {tmp_table} (\\n\\t{names}\\n);\\n\\n' \\\n",
    "                                .format(tmp_table=tmp_table, names=names)\n",
    "        \n",
    "        print(sql)\n",
    "        \n",
    "        return sql\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _copy_from(self, file_or_buffer, from_table, columns=None, sep=','):\n",
    "        \n",
    "        con = self.__connect()\n",
    "        \n",
    "        # Run update query\n",
    "        #data_buffer = StringIO(data.to_csv(header=False, index=False))\n",
    "        \n",
    "        try:\n",
    "            cur = con.cursor()\n",
    "            # Copy into temp_table\n",
    "            #cur.copy_from(file=data_buffer.read(), table=from_table, columns=columns, sep=sep)\n",
    "            #data_buffer.close()\n",
    "            #con.commit()\n",
    "            cur.close()\n",
    "            print('Updated table \"{}\".'.format(self.table))\n",
    "        except Exception as e:\n",
    "            con.rollback()\n",
    "            print('Error:\\n', e)\n",
    "        finally:\n",
    "            if con is not None:\n",
    "                con.close()\n",
    "                \n",
    "        return\n",
    "                \n",
    "        \n",
    "        \n",
    "\n",
    "    def _update_table_from(self, temp_table, columns=None):\n",
    "        \n",
    "        # CREATE TABLE query\n",
    "        #table = \"tmp_\" + self.table\n",
    "        \n",
    "        column_names = self.get_names().tolist() if not columns else columns\n",
    "        \n",
    "        sql_update = 'UPDATE {table}\\n'.format(table=self.table)\n",
    "        sql_set = [\"SET \"]\n",
    "        \n",
    "        for name in column_names:\n",
    "            set_sql = \"{name} = {tmp_name},\\n\\t\".format(name=name, tmp_name=temp_table + '.' + name)\n",
    "            sql_set.append(set_sql)\n",
    "\n",
    "        sql_set = ''.join(sql_set)\n",
    "        sql_set = sql_set[:-3] + \"\\n\"\n",
    "\n",
    "        sql_from = \"FROM {tmp_table}\\nWHERE {db_table}.{id_col} = {tmp_table}.{id_col};\\n\\n\" \\\n",
    "                            .format(tmp_table=tmp_table, db_table=self.table, id_col=id_col)\n",
    "        sql_drop = 'DROP TABLE {};\\n'.format(tmp_table)\n",
    "                \n",
    "        sql = sql_update + sql_set + sql_from + sql_drop\n",
    "        \n",
    "        print(sql)\n",
    "        \n",
    "        return sql\n",
    "            \n",
    "                \n",
    "    def _run_query(self, sql):\n",
    "        \n",
    "        con = self.__connect()\n",
    "            \n",
    "        try:\n",
    "            cur = con.cursor()\n",
    "            cur.execute(sql)\n",
    "            con.commit()\n",
    "            cur.close()\n",
    "            print('Updated database \"{}\".'.format(self.dbname))\n",
    "        except Exception as e:\n",
    "            con.rollback()\n",
    "            print('Error:\\n', e)\n",
    "        finally:\n",
    "            if con is not None:\n",
    "                con.close()\n",
    "                \n",
    "                \n",
    "            \n",
    "    # Builds a query to update postgres from a csv file\n",
    "    def update_values(self, data, id_col, types_dict, columns=None, sep=','):\n",
    "        \n",
    "        # Fetch data types\n",
    "        #types_dict = self.get_types()\n",
    "        #types_dict = types_dict\n",
    "        # Append id_col to selected columns\n",
    "        #columns = None if not columns else [id_col] + columns\n",
    "        \n",
    "        # CREATE TABLE query\n",
    "        #tmp_table = \"tmp_\" + self.table\n",
    "\n",
    "        #column_names = self.get_names().tolist() if not columns else columns\n",
    "        \n",
    "        # Subsets types_dict by columns argument and formats into string\n",
    "        #types_dict = types_dict if not columns else {key:value for key, value in types_dict.items() if key in set(columns)}\n",
    "        #names = ',\\n\\t'.join(['{key} {val}'.format(key=key, val=val) for key, val in types_dict.items()])\n",
    "        \n",
    "        # Build queries\n",
    "        #sql_create_tmp_table = 'DROP TABLE IF EXISTS {};\\n\\n'.format(tmp_table)\n",
    "        #sql_create_tmp_table = sql_create_tmp_table + 'CREATE TABLE {tmp_table} (\\n\\t{names}\\n);\\n\\n' \\\n",
    "                                #.format(tmp_table=tmp_table, names=names)\n",
    "           \n",
    "        #sql_update_query = 'UPDATE {db_table}\\n'.format(db_table=self.table)\n",
    "        \n",
    "        #sql_set = [\"SET \"]\n",
    "        \n",
    "        #for name in column_names:\n",
    "            #set_sql = \"{name} = {tmp_name},\\n\\t\".format(name=name, tmp_name=tmp_table + '.' + name)\n",
    "            #sql_set.append(set_sql)\n",
    "            \n",
    "        #sql_set = ''.join(sql_set)\n",
    "        #sql_set = sql_set[:-3] + \"\\n\"\n",
    "        \n",
    "        #sql_from = \"FROM {tmp_table}\\nWHERE {db_table}.{id_col} = {tmp_table}.{id_col};\\n\\n\" \\\n",
    "                            #.format(tmp_table=tmp_table, db_table=self.table, id_col=id_col)\n",
    "        #sql_drop = 'DROP TABLE {};\\n'.format(tmp_table)\n",
    "        \n",
    "        #sql_query_1 = sql_create_tmp_table\n",
    "        #sql_query_2 = sql_update_query + sql_set + sql_from + sql_drop\n",
    "\n",
    "        # Preview sql query to debug\n",
    "        #print(sql_query_1 + \"# Copy into temp_table\\ncur.copy_from(...)\\n\\n\"+ sql_query_2)\n",
    "        \n",
    "        # Run update query\n",
    "        data_buffer = StringIO(data.to_csv(header=False, index=False))\n",
    "        con = self.__connect()\n",
    "        try:\n",
    "            cur = con.cursor()\n",
    "            \n",
    "            # Create tmp_table\n",
    "            #cur.execute(sql_query_1)\n",
    "            # Copy into temp_table\n",
    "            #data_buffer.read()\n",
    "            #cur.copy_from(file=data_buffer, table=tmp_table, columns=columns, sep=sep)\n",
    "            #data_buffer.close()\n",
    "            \n",
    "            # Update from temp_table into table and delete temp\n",
    "            #cur.execute(sql_query_2)\n",
    "            #con.commit()\n",
    "            cur.close()\n",
    "            print('Updated table \"{}\".'.format(self.table))\n",
    "        except Exception as e:\n",
    "            con.rollback()\n",
    "            print('Error:\\n', e)\n",
    "        finally:\n",
    "            if con is not None:\n",
    "                con.close()\n",
    "                \n",
    "####Update types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "permits = Table(table=\"permits_raw\")\n",
    "data = permits.fetch_data(sql=\"SELECT * FROM permits_raw;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS tmp_permits_raw;\n",
      "\n",
      "CREATE TABLE tmp_permits_raw (\n",
      "\tassessor_book SMALLINT,\n",
      "\tpcis_permit_no VARCHAR(50),\n",
      "\tinitiating_office VARCHAR(50)\n",
      ");\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "permits._create_temp_table(types_dict=types_dict, id_col=\"pcis_permit_no\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:\n",
      " column \"assessor_book\" of relation \"permits_raw\" already exists\n",
      "\n"
     ]
    }
   ],
   "source": [
    "permits.reformat_names(replace_map, update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = permits.fetch_data(sql=\"SELECT * FROM permits_raw;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_column_full_address(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for geocoding 19 addresses is $0.10.\n",
      "Geocoding...\n",
      "19 locations were assigned coordinates.\n"
     ]
    }
   ],
   "source": [
    "geocode_latitude_longitude(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = split_column_lat_long(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table columns are already up to date.\n"
     ]
    }
   ],
   "source": [
    "permits.add_columns(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rearranged dataframe columns to match table \"permits_raw\".\n"
     ]
    }
   ],
   "source": [
    "# Move inside of update_values\n",
    "data = permits.match_column_order(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS tmp_permits_raw;\n",
      "\n",
      "CREATE TABLE tmp_permits_raw (\n",
      "\tassessor_book SMALLINT,\n",
      "\tpcis_permit_no VARCHAR(50),\n",
      "\tlatitude NUMERIC\n",
      ");\n",
      "\n",
      "# Copy into temp_table\n",
      "cur.copy_from(...)\n",
      "\n",
      "UPDATE permits_raw\n",
      "SET pcis_permit_no = tmp_permits_raw.pcis_permit_no,\n",
      "\tassessor_book = tmp_permits_raw.assessor_book,\n",
      "\tlatitude = tmp_permits_raw.latitude\n",
      "FROM tmp_permits_raw\n",
      "WHERE permits_raw.pcis_permit_no = tmp_permits_raw.pcis_permit_no;\n",
      "\n",
      "DROP TABLE tmp_permits_raw;\n",
      "\n",
      "Updated table \"permits_raw\".\n"
     ]
    }
   ],
   "source": [
    "permits.update_values(data, id_col=\"pcis_permit_no\", types_dict=types_dict, columns=['assessor_book', 'latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
