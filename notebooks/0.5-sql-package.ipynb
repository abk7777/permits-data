{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Package\n",
    "\n",
    "Provides simple functionality to interact with a PostgreSQL server using Python classes.\n",
    "\n",
    "**Overview of functionality:**\n",
    "* Database(self, user, password, host, dbname, port)\n",
    "    * properties\n",
    "        * user\n",
    "        * password\n",
    "        * host\n",
    "        * dbname\n",
    "        * port\n",
    "    * methods\n",
    "        * create(name) x\n",
    "        * connect()\n",
    "        * drop(name)\n",
    "* Table(self, dbname, table, schema)\n",
    "    * accepts db properties\n",
    "    * properties\n",
    "        * connect() --> inherited\n",
    "        * fetch_data(sql, con, parse_dates)\n",
    "        * get_names()\n",
    "        * format_names(char_dict)\n",
    "        * update_names(names_dict)\n",
    "        * add_columns(columns_list, type=None)\n",
    "        * compare_column_order(dataframe)\n",
    "        * match_columns(dataframe)\n",
    "        * save_csv(data, local_path, match_column_order=True)\n",
    "        * update_values(local_path, container_path)\n",
    "        * update_types(types_dict)\n",
    "        * close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "#sys.path[0] = str(Path(__file__).resolve().parents[2]) # Set path for custom modules\n",
    "import warnings\n",
    "from io import StringIO\n",
    "\n",
    "# Set path for modules\n",
    "sys.path[0] = '../'\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# SQL libraries\n",
    "import psycopg2\n",
    "from src.pipeline.dictionaries import types_dict, replace_map\n",
    "from src.pipeline.transform_data import create_column_full_address, geocode_latitude_longitude, split_column_lat_long\n",
    "from src.toolkits.sql import format_names\n",
    "\n",
    "# Set notebook display options\n",
    "pd.set_option('display.max_rows', 2000)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Get project root directory\n",
    "root_dir = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Database():\n",
    "    \n",
    "    # if modulename not in sys.modules: print...\n",
    "    load_dotenv(find_dotenv());\n",
    "    \n",
    "    def __init__(self, user=None, password=None,\n",
    "                 dbname=None, host=None, port=None):\n",
    "        \n",
    "        # Loaded from .env if not explicit\n",
    "        self.user = user if user is not None else os.getenv(\"POSTGRES_USER\")\n",
    "        self.password = password if password is not None else os.getenv(\"POSTGRES_PASSWORD\")\n",
    "        self.dbname = dbname if dbname is not None else os.getenv(\"POSTGRES_DB\")\n",
    "        self.host = host if host is not None else os.getenv(\"DB_HOST\")\n",
    "        self.port = port if port is not None else os.getenv(\"DB_PORT\")\n",
    "        \n",
    "        \n",
    "        # Root directory\n",
    "        self._root_dir = os.path.dirname(os.getcwd())\n",
    "        #sys.path[0] = str(Path(__file__).resolve().parents[2])\n",
    "        \n",
    "    def _connect(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Connects to PostgreSQL database using psycopg2 driver. Same\n",
    "        arguments as psycopg2.connect().\n",
    "\n",
    "        Params\n",
    "        --------\n",
    "        dbname\n",
    "        user\n",
    "        password\n",
    "        host\n",
    "        port\n",
    "        connect_timeout\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            con = psycopg2.connect(dbname=self.dbname,\n",
    "                                   user=self.user,\n",
    "                                   password=self.password,\n",
    "                                    host=self.host, \n",
    "                                    port=self.port,\n",
    "                                  connect_timeout=3)            \n",
    "        except Exception as e:\n",
    "            print('Error:', e)\n",
    "            return None\n",
    "\n",
    "        return con\n",
    "    \n",
    "    @property\n",
    "    def _con(self):\n",
    "        try:\n",
    "            con = self._connect()\n",
    "            print('Connected as user \"{}\" to database \"{}\" on http://{}:{}.'.format(self.user,self.dbname,\n",
    "                                                               self.host,self.port))\n",
    "            con.close()\n",
    "        except Exception as e:\n",
    "            con.rollback()\n",
    "            print('Error:', e)\n",
    "        finally:\n",
    "            if con is not None:\n",
    "                con.close()\n",
    "                \n",
    "                \n",
    "    def _run_query(self, sql):\n",
    "        \n",
    "        try:\n",
    "            con = self._connect()\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)            \n",
    "            \n",
    "        try:\n",
    "            cur = con.cursor()\n",
    "            cur.execute(sql)\n",
    "            con.commit()\n",
    "            cur.close()\n",
    "            print('Query successful on database \"{}\".'.format(self.dbname))\n",
    "        except Exception as e:\n",
    "            con.rollback()\n",
    "            print(\"Error:\", e)\n",
    "        finally:\n",
    "            if con is not None:\n",
    "                con.close()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def create_table(self, table_name, types_dict, id_col, columns=None):\n",
    "        \n",
    "        # Append id_col to selected columns\n",
    "        columns = None if not columns else set([id_col] + columns)\n",
    "        \n",
    "        # Subsets types_dict by columns argument and formats into string if no columns are specified\n",
    "        types_dict = types_dict if not columns else {key:value for key, value in types_dict.items() if key in set(columns)}\n",
    "        names = ',\\n\\t'.join(['{key} {val}'.format(key=key, val=val) for key, val in types_dict.items()])\n",
    "        \n",
    "        # Build queries\n",
    "        sql = 'CREATE TABLE {table_name} (\\n\\t{names}\\n);\\n\\n' \\\n",
    "                            .format(table_name=table_name, names=names) # + sql\n",
    "        \n",
    "        # Execute query\n",
    "        self._run_query(sql)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def drop_table(self, table_name):\n",
    "        \n",
    "        # Build queries\n",
    "        sql = 'DROP TABLE IF EXISTS {table_name};\\n\\n'.format(table_name=table_name)\n",
    "        \n",
    "        # Execute query\n",
    "        self._run_query(sql)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "\n",
    "    def _create_temp_table(self, types_dict, id_col, columns=None):\n",
    "        \n",
    "        # Append id_col to selected columns\n",
    "        columns = None if not columns else [id_col] + columns\n",
    "        \n",
    "        # CREATE TABLE query\n",
    "        tmp_table = \"tmp_\" + self.table\n",
    "        \n",
    "        # Subsets types_dict by columns argument and formats into string if no columns are specified\n",
    "        types_dict = types_dict if not columns else {key:value for key, value in types_dict.items() if key in set(columns)}\n",
    "        names = ',\\n\\t'.join(['{key} {val}'.format(key=key, val=val) for key, val in types_dict.items()])\n",
    "        \n",
    "        # Build queries\n",
    "        sql = 'DROP TABLE IF EXISTS {tmp_table};\\n\\n'.format(tmp_table=tmp_table)\n",
    "        sql = sql + 'CREATE TABLE {tmp_table} (\\n\\t{names}\\n);\\n\\n' \\\n",
    "                                .format(tmp_table=tmp_table, names=names)\n",
    "        \n",
    "        # Execute query\n",
    "        self._run_query(sql)\n",
    "        \n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Table(Database):\n",
    "    def __init__(self, user=None, password=None, dbname=None, host=None, port=None, table=None):\n",
    "        super().__init__(user, password, dbname, host, port)\n",
    "        \n",
    "        self.table = table\n",
    "        \n",
    "        # Loaded from .env if not explicit\n",
    "        self.user = user if user is not None else os.getenv(\"POSTGRES_USER\")\n",
    "        self.password = password if password is not None else os.getenv(\"POSTGRES_PASSWORD\")\n",
    "        self.dbname = dbname if dbname is not None else os.getenv(\"POSTGRES_DB\")\n",
    "        self.host = host if host is not None else os.getenv(\"DB_HOST\")\n",
    "        self.port = port if port is not None else os.getenv(\"DB_PORT\")\n",
    "\n",
    "    \n",
    "    # Connect to database\n",
    "    def __connect(self):\n",
    "        return super(Table, self)._connect()\n",
    "    \n",
    "    # Check info on connection\n",
    "    def __con(self):\n",
    "        return super(Table, self)._con\n",
    "    \n",
    "    # Run query\n",
    "    def __run_query(self, sql):\n",
    "        return super(Table, self)._run_query(sql)\n",
    "    \n",
    "    # Run query\n",
    "    def __create_temp_table(self, types_dict, id_col, columns):\n",
    "        return super(Table, self)._create_temp_table(types_dict, id_col, columns)\n",
    "    \n",
    "    \n",
    "    # Fetch data from sql query\n",
    "    def fetch_data(self, sql, coerce_float=False, parse_dates=None):\n",
    "        \n",
    "        con = self.__connect()\n",
    "        \n",
    "        # Fetch fresh data\n",
    "        data = pd.read_sql_query(sql=sql, con=con, coerce_float=coerce_float, parse_dates=parse_dates)\n",
    "\n",
    "        # Replace None with np.nan\n",
    "        data.fillna(np.nan, inplace=True)\n",
    "        \n",
    "        # Close db connection\n",
    "        con.close()\n",
    "\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    # Get names of column\n",
    "    def get_names(self):\n",
    "        \n",
    "        # Specific query to retrieve table names\n",
    "        sql = \"SELECT * FROM information_schema.columns WHERE table_name = N'{}'\".format(self.table)\n",
    "        \n",
    "        # Run query and extract\n",
    "        try:\n",
    "            con = self.__connect()\n",
    "            data = pd.read_sql_query(sql, con)\n",
    "            column_series = data['column_name']\n",
    "            con.close()\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "    \n",
    "        return column_series\n",
    "\n",
    "    \n",
    "    # Get types of columns, returns dict\n",
    "    def get_types(self, as_dataframe=False):\n",
    "        \n",
    "        # Specific query to retrieve table names\n",
    "        sql = '''SELECT column_name, \n",
    "        CASE \n",
    "            WHEN domain_name is not null then domain_name\n",
    "            WHEN data_type='character varying' THEN 'varchar('||character_maximum_length||')'\n",
    "            WHEN data_type='character' THEN 'char('||character_maximum_length||')'\n",
    "            WHEN data_type='numeric' THEN 'numeric'\n",
    "            ELSE data_type\n",
    "        END AS type\n",
    "        FROM information_schema.columns WHERE table_name = 'permits_raw';\n",
    "        '''\n",
    "        \n",
    "        # Run query and extract\n",
    "        try:\n",
    "            con = self.__connect()\n",
    "            data = pd.read_sql_query(sql, con)\n",
    "            con.close()\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "        \n",
    "        if as_dataframe:\n",
    "            data['type'] = data['type'].str.upper()\n",
    "            return data\n",
    "        \n",
    "        types_dict = dict(zip(data['column_name'], data['type'].str.upper()))\n",
    "        \n",
    "        return types_dict\n",
    "    \n",
    "    \n",
    "    # Update column names in db table\n",
    "    def _update_table_names(self, series):\n",
    "\n",
    "        # Extract current columns in table\n",
    "        old_columns = self.get_names()\n",
    "\n",
    "        # Create list of reformatted columns to replace old columns \n",
    "        new_columns = series\n",
    "\n",
    "        # SQL query string to change column names\n",
    "        sql = 'ALTER TABLE {} '.format(self.table) + 'RENAME \"{old_name}\" to {new_name};'\n",
    "\n",
    "        sql_query = []\n",
    "\n",
    "        # Iterate through old column names and replace each with reformatted name \n",
    "        for idx, name in old_columns.iteritems():\n",
    "            sql_query.append(sql.format(old_name=name, new_name=new_columns[idx]))\n",
    "\n",
    "        # Join list to string\n",
    "        sql_query = '\\n'.join(sql_query)\n",
    "\n",
    "        return sql_query\n",
    "    \n",
    "\n",
    "    # Standardize column names using dictionary of character replacements\n",
    "    def format_table_names(self, replace_map, update=False):\n",
    "        \n",
    "        series = self.get_names()\n",
    "        \n",
    "        def replace_chars(text):\n",
    "            for oldchar, newchar in replace_map.items():\n",
    "                text = text.replace(oldchar, newchar).lower()\n",
    "            return text\n",
    "        \n",
    "        series = series.apply(replace_chars)  \n",
    "        \n",
    "        if not update:\n",
    "            warning.warn('No changes made. Set \"update=False\" to run query on database.')\n",
    "            return series.apply(replace_chars)\n",
    "        \n",
    "        else:\n",
    "            sql_query = self._update_table_names(series=series)\n",
    "            \n",
    "            # Execute query\n",
    "            self.__run_query(sql_query)\n",
    "            \n",
    "            return self\n",
    "                    \n",
    "\n",
    "    # Add new columns to database\n",
    "    def add_columns_from_data(self, data):\n",
    "        \n",
    "        # Get names of current columns in PostgreSQL table\n",
    "        current_names = self.get_names().tolist()\n",
    "\n",
    "        # Get names of updated table not in current table\n",
    "        updated_names = data.columns.tolist()\n",
    "        new_names = list(set(updated_names) - set(current_names))\n",
    "\n",
    "        # Check names list is not empty\n",
    "        if not new_names:\n",
    "            print(\"Table columns are already up to date.\")\n",
    "            return\n",
    "\n",
    "        # Format strings for query\n",
    "        alter_table_sql = \"ALTER TABLE {db_table}\\n\"\n",
    "        add_column_sql = \"\\tADD COLUMN {column} TEXT,\\n\"\n",
    "\n",
    "        # Create a list and append ADD column statements\n",
    "        sql_query = [alter_table_sql.format(db_table=self.table)]\n",
    "        for name in new_names:\n",
    "            sql_query.append(add_column_sql.format(column=name))\n",
    "\n",
    "        # Join into one string\n",
    "        sql = ''.join(sql_query)[:-2] + \";\"\n",
    "\n",
    "        # Execute query\n",
    "        self.__run_query(sql)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    # Check whether dataframe columns match database table columns before running queries\n",
    "    def _match_column_order(self, data):\n",
    "        \n",
    "        # Get columns from database as list\n",
    "        db_columns = self.get_names().tolist()\n",
    "\n",
    "        # Select columns from dataframe as list\n",
    "        data_columns = data.columns.tolist()\n",
    "        \n",
    "        if set(data_columns) == set(db_columns):\n",
    "            if data_columns != db_columns:\n",
    "                print('Rearranged dataframe columns to match table \"{}\".'.format(self.table))\n",
    "                data = data[db_columns]\n",
    "                return True\n",
    "            else:\n",
    "                print('Dataframe columns already match table \"{}\".'.format(self.table))\n",
    "                return True\n",
    "        else:\n",
    "            if len(data_columns) > len(db_columns):\n",
    "                print('Dataframe has columns not in table \"{}\":'.format(self.table))\n",
    "                print(list(set(data_columns) - set(db_columns)))\n",
    "                return False\n",
    "            else:\n",
    "                print('Dataframe missing columns that are in table \"{}\":'.format(self.table))\n",
    "                print(list(set(db_columns) - set(data_columns)))\n",
    "                return False\n",
    "        \n",
    "    \n",
    "    def _copy_from_dataframe(self, data, id_col, columns=None):\n",
    "        \n",
    "        if self._match_column_order(data):\n",
    "        \n",
    "            try:\n",
    "                con = self.__connect()\n",
    "            except Exception as e:\n",
    "                print(\"Error:\", e)\n",
    "            \n",
    "            columns = data.columns.tolist() if not columns else columns\n",
    "            temp_table = \"tmp_\" + self.table\n",
    "            data_buffer = StringIO(data.to_csv(header=False, index=False, sep=','))\n",
    "\n",
    "            try:\n",
    "                cur = con.cursor()\n",
    "                data_buffer.read()\n",
    "                cur.copy_from(file=data_buffer, table=temp_table, columns=columns)\n",
    "                data_buffer.close()\n",
    "                con.commit()\n",
    "                cur.close()\n",
    "                print('Copy successful on table \"{}\".'.format(self.table))\n",
    "            except Exception as e:\n",
    "                con.rollback()\n",
    "                print(\"Error:\", e)\n",
    "            finally:\n",
    "                if con is not None:\n",
    "                    con.close()\n",
    "                \n",
    "        return self\n",
    "                \n",
    "        \n",
    "    def _update_from_temp(self, id_col, columns=None):\n",
    "        \n",
    "        temp_table = \"tmp_\" + self.table\n",
    "        columns = self.get_names().tolist() if not columns else columns\n",
    "        sql_update = 'UPDATE {table}\\n'.format(table=self.table)\n",
    "        sql_set = [\"SET \"]\n",
    "        \n",
    "        for name in columns:\n",
    "            line = \"{name} = {tmp_name},\\n\\t\".format(name=name, tmp_name=temp_table + '.' + name)\n",
    "            sql_set.append(line)\n",
    "\n",
    "        sql_set = ''.join(sql_set)\n",
    "        sql_set = sql_set[:-3] + \"\\n\"\n",
    "\n",
    "        sql_from = \"FROM {temp_table}\\nWHERE {this_table}.{id_col} = {temp_table}.{id_col};\\n\\n\" \\\n",
    "                            .format(temp_table=temp_table, this_table=self.table, id_col=id_col)\n",
    "        sql_drop = 'DROP TABLE {};\\n'.format(temp_table)\n",
    "                \n",
    "        sql = sql_update + sql_set + sql_from + sql_drop\n",
    "        \n",
    "        # Execute query\n",
    "        self.__run_query(sql)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "                \n",
    "    # Builds a query to update postgres from a csv file\n",
    "    def update_values(self, data, id_col, types_dict, columns=None, sep=','):\n",
    "        \n",
    "        columns = self.get_names().tolist() if not columns else [id_col] + columns\n",
    "        params = {\"id_col\":id_col, \"columns\":columns}\n",
    "        \n",
    "        self.__create_temp_table(types_dict=types_dict, **params) \\\n",
    "                        ._copy_from_dataframe(data=data, **params) \\\n",
    "                        ._update_from_temp(**params)\n",
    "        \n",
    "                \n",
    "####Update types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "permits = Table(table=\"permits_raw\")\n",
    "data = permits.fetch_data(sql=\"SELECT * FROM permits_raw;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"columns\":['assessor_book', 'latitude'], \"id_col\":\"pcis_permit_no\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS tmp_permits_raw;\n",
      "\n",
      "CREATE TABLE tmp_permits_raw (\n",
      "\tassessor_book SMALLINT,\n",
      "\tpcis_permit_no VARCHAR(50),\n",
      "\tlatitude NUMERIC\n",
      ");\n",
      "\n",
      "\n",
      "Query successful on database \"permits\".\n",
      "Dataframe columns already match table \"permits_raw\".\n",
      "Copy successful on table \"permits_raw\".\n",
      "Query successful on database \"permits\".\n"
     ]
    }
   ],
   "source": [
    "permits.update_values(data, types_dict=types_dict, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query successful on database \"permits\".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Table at 0x11592a5d0>"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permits._create_temp_table(types_dict=types_dict, id_col=\"pcis_permit_no\", columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe columns already match table \"permits_raw\".\n",
      "Copy successful on table \"permits_raw\".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Table at 0x11592a5d0>"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permits._copy_from_dataframe(data, id_col=\"pcis_permit_no\", columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query successful on database \"permits\".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Table at 0x11592a5d0>"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permits._update_from_temp(id_col=\"pcis_permit_no\", columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: column \"assessor_book\" of relation \"permits_raw\" already exists\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Table at 0x11592a5d0>"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permits.format_table_names(replace_map, update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = permits.fetch_data(sql=\"SELECT * FROM permits_raw;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_column_full_address(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for geocoding 19 addresses is $0.10.\n",
      "Geocoding...\n",
      "19 locations were assigned coordinates.\n"
     ]
    }
   ],
   "source": [
    "geocode_latitude_longitude(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = split_column_lat_long(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table columns are already up to date.\n"
     ]
    }
   ],
   "source": [
    "permits.add_columns_from_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rearranged dataframe columns to match table \"permits_raw\".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move inside of update_values\n",
    "permits._match_column_order(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
